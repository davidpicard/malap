{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP DT\n",
    "\n",
    "Save the notebook as either PDF or HTML and make sure all the results are saved correctly (I won't run them and the original format does not save the results automatically), **and put your name in the filename**.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "<b>Questions are in green boxes.</b>\n",
    "The maximum time you should spend on each question is given as indication only. If you take more time than that, then you should come see me.\n",
    "</div>\n",
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyzes are in blue boxes.</b> You should comment on your results in theses boxes (Is it good? Is it expected? Why do we get such result? Why is it different from the previous one? etc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will use the bluebell dataset. It consists of $64\\times 64$ color images, which we will have to flatten into $12k$ dimensional vectors. The code for the dataset comes with several train/val/test splits, but in this notebook, we will use the first split and do our own cross-validation routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from bluebell import Bluebell\n",
    "X_train_ds = Bluebell('bluebell_64', 'train', split=0)\n",
    "X_val_ds = Bluebell('bluebell_64', 'val', split=0)\n",
    "X_train = np.array([img.flatten()/127.5 - 1. for img, lab in X_train_ds])\n",
    "y_train = np.array([lab for img, lab in X_train_ds])\n",
    "X_val = np.array([img.flatten()/127.5 - 1. for img, lab in X_val_ds])\n",
    "y_val = np.array([lab for img, lab in X_val_ds])\n",
    "plt.imshow(X_train[0].reshape(64, 64, 3)/2+0.5)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to reduce the number of dimensions that will be search through with the decision trees. Since our images are $64\\times 64\\times 3$ values, this leads to a very high dimensional space that has to be searched at each step. However, dimensions where all images have the same value, or very close values, will never be selected in the tree because they do not provide a good gain.\n",
    "\n",
    "We will thus select only the 2048 dimension with the highest variance to perform our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = jnp.argsort(X_train.std(axis=0), descending=True)[0:2048]\n",
    "X_train = jnp.array(X_train[:, dim])\n",
    "X_val = jnp.array(X_val[:, dim])\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a randomized Decision Tree\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q1.</b> Implement the code of a function that finds an optimal threshold along a given dimension, using the $0-1$ loss with specified example weights and test it on the 150th dimension. To speed-up things, we will only consider 8 thresholds between the minimum and maximum value (use 'linspace'). Compare it to assigning a unique label to all samples. You should get a significant decrease of loss from ~0.92 (random 1/12 chance) to ~0.84. <i>(Indicative time: 10 minutes for a slow version, but take the extra 20 minutes to have a parallel version testing all thresholds at once that runs in under 1s, it is worth it for the next questions.)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes arguements \n",
    "y_pred: prediction\n",
    "y_true: true labels\n",
    "weights: weights for each example\n",
    "'''\n",
    "@jax.jit\n",
    "def zeroOneLoss(y_pred, y_true, weights):\n",
    "    return (weights * (y_pred != y_true)).sum(axis=0)/(1e-12+weights.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes arguments\n",
    "X: training samples\n",
    "y: training labels\n",
    "dim: dimension to use\n",
    "w: weight associated to each example (can be True/False or 1/0 to remove some examples)\n",
    "returns the gain and the threshold\n",
    "'''\n",
    "@jax.jit\n",
    "def findBestTh(X, y, dim, w):\n",
    "    \n",
    "    return gain, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "G, th = findBestTh(X_train, y_train, 150, jnp.ones(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vectorize over the dimension by using vmap. The batched function can now operate on a array of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_findBestTh = jax.vmap(findBestTh, in_axes=(None, None, 0, None), out_axes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q2.</b> Wrap the batched function in a function that test all dimensions to find the best combination of component and threshold. Use blocks of 256 dimensions to process at a time, as we found it a good setup with respect to speed (You can change those values later to optimize for speed).Test it on the entire train set and make sure it obtains the lowest error.<i>(Indicative time: It could take you 15 minutes to an hour to code and should run in less than 5 seconds.)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def findBestDTh(X, y, w):\n",
    "    return best_gain, best_dim, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g, d, t = findBestDTh(X_train, y_train, jnp.ones(len(X_train)))\n",
    "print(\"gain: {} dim: {} th: {}\".format(g, d, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q3.</b> Implement the code of the Decision Tree class by using the previous functions. To achieve reasonable speed, loop only over dimensions that have variations (there is no threshold if all samples have the same value) in batches of 256 or more, inspired by the previous function. Do not split and slice the data but zero the associated weights instead, it is faster (all functions have the same size of arrays and are thus compiled and optimized only once). Debug it on only 256 dimensions and 256 samples, because using all dimensions/samples takes about 2 minutes. Test it on the full set with a maximum depth of 8 and a leaf size less than 10 to analyze and comment. <i>(Indicative time: It could take you 30 minutes to an hour to code and debug since it involves recursion.)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomizedDT():\n",
    "    '''\n",
    "    percent_dimension: percent of dimensions to use (random selection)\n",
    "    '''\n",
    "    def __init__(self, percent_dimension=1.0, max_depth=8, max_size=20, verbose=False, space=0):\n",
    "        self.percent_dimension = percent_dimension\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    '''\n",
    "    train this decision tree on a random subset of dimensions (columns) of X with a maximum depth\n",
    "    '''\n",
    "    def fit(self, X, y, w=None):\n",
    "        \n",
    "        return       \n",
    "    \n",
    "    '''\n",
    "    predict the set of samples\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        if self.label is not None:\n",
    "            return self.label * jnp.ones(len(X))\n",
    "        return jnp.concatenate([self.left.predict([x]) if x[self.dim] < self.th else self.right.predict([x]) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try on the training set reduced to digits 0 and 1 with all dimensions to check that our code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train_01 = X_train\n",
    "y_train_01 = y_train\n",
    "x_val_01 = X_val\n",
    "y_val_01 = y_val\n",
    "\n",
    "\n",
    "x_train_01 = x_train_01[0:256, 0:256]\n",
    "y_train_01 = y_train_01[0:256]\n",
    "x_val_01 = x_val_01[0:100, 0:256]\n",
    "y_val_01 = y_val_01[0:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q4.</b> Use cross-validation on the full digit dataset (0-9) to select a reasonnable depth between 2 and 8, using random splits of half the training set to save on training time. <i>(Indicative time: maximum 10 minutes to code, about 20 minutes to run)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Next, we want to mitigate the tendancy of decision trees to overfit when the depth is too high and to underfit when the depth is too small by implementing random forests.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q5.</b> Code a Random Forest of decision trees, each trained on a subset of the training set. Perform a corase cross-validation to set a reasonnable number of trees (25, 50, 75), percent of training data used (0.5, 0.75), percent of dimensions used (0.5, 0.75) and depth 3. <i>(Indicative time: less than 20 minutes to code, takes more than 20 minutes to run)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, nb_trees, percent_dataset=1., percent_dimension=1., max_depth=8):\n",
    "        self.nb_trees = nb_trees\n",
    "        self.percent_dataset = percent_dataset\n",
    "        self.percent_dimension = percent_dimension\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        for dt in self.trees:\n",
    "            y.append(dt.predict(X))\n",
    "        y = jax.nn.one_hot(jnp.array(y), num_classes=12)\n",
    "        return y.sum(axis=0).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "To have a more efficient training procedure, we will remove the independance between the trees by using boosting\n",
    "\n",
    "<div class=\"alert alert-warning\"> \n",
    "    <b>Q6.</b> Code the BoostingClassifier that obtains a combination of Randomized Trees using AdaBoost. Each tree is trained using the weighted $0-1$ loss. To allow the tree combination, convert the output of each tree to a one-hot encoded vector. The output of the boosted trees is then the weighted sum of these one-hot vectors and the predicted class is the argmax. Test with the same parameters as the best Random Forest. <i>(Indicative time: about 30 minutes to code, runs about as fast as a random forest)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedTrees():\n",
    "    def __init__(self, nb_trees, percent_dataset=1., percent_dimension=1., max_depth=8):\n",
    "        self.nb_tress = nb_trees\n",
    "        self.percent_dataset = percent_dataset\n",
    "        self.percent_dimension = percent_dimension\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "In order to visualize the decision, we can produce an image the contains only the relevant information with respect to the decisions taken by a tree.\n",
    "\n",
    "<div class=\"alert alert-warning\"> \n",
    "    <b>Q7.</b> For a trained tree, select a leaf and build an image that has a value of 1 for each pixel in the decision path that should be above the threshold, 0 for each pixels in the decision path that should be below the threshold and 0.5 everywhere else. For all classes, show an average all such images for each leaf corresponding to that class. <i>(Indicative time: about one hour to code)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
