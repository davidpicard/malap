{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Metric learning\n",
    "\n",
    "Save the notebook as either PDF or HTML and make sure all the results are saved correctly (I won't run them and the original format does not save the results automatically), **and put your name in the filename**.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "<b>Questions are in green boxes.</b>\n",
    "The maximum time you should spend on each question is given as indication only. If you take more time than that, then you should come see me.\n",
    "</div>\n",
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyzes are in blue boxes.</b> You should comment on your results in theses boxes (Is it good? Is it expected? Why do we get such result? Why is it different from the previous one? etc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will use the bluebell dataset. It consists of $64\\times 64$ color images, which we will have to flatten into $12k$ dimensional vectors. The code for the dataset comes with several train/val/test splits, but in this notebook, we will use the first split and do our own cross-validation routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from bluebell import Bluebell\n",
    "X_train_ds = Bluebell('bluebell_64', 'train', split=0)\n",
    "X_val_ds = Bluebell('bluebell_64', 'val', split=0)\n",
    "X_train = np.array([img.flatten()/127.5 - 1. for img, lab in X_train_ds])\n",
    "y_train = np.array([lab for img, lab in X_train_ds])\n",
    "X_val = np.array([img.flatten()/127.5 - 1. for img, lab in X_val_ds])\n",
    "y_val = np.array([lab for img, lab in X_val_ds])\n",
    "plt.imshow(X_train[0].reshape(64, 64, 3)/2+0.5)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic distance class\n",
    "\n",
    "We start with a generic trainable distance class that serves as an interface for all the different distances we will implement. The `fit` method does the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance():\n",
    "    '''\n",
    "    trains this distance function on a training set\n",
    "    '''\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    returns the distance between the sets X1 and X2:\n",
    "    X1 is n x d (n samples of dimension d)\n",
    "    X2 is m x d (n samples of dimension d)\n",
    "    output is n x m (distance matrix)\n",
    "    '''\n",
    "    def predict(self, X1, X2):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a $k$-Nearest-Neighbor\n",
    "\n",
    "<div class=\"alert alert-success\"> <b>Q1.</b> Implement a class that encapsulate the squared euclidean distance ($\\| x_1 - x_2 \\|^2$) using the Distance parent class. In this case, the `fit` method does nothing. <i>(Indicative time: about 10 minutes to code)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Distance(Distance):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print('fitting...')\n",
    "        print('fitted!')\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> <b>Q2.</b> Implement a $k$-NearestNeighbor class that relies on a Distance object to find the neighbors. It also trains the distance. Test it using your L2 Distance with $k=36$ (which should give about the same train and validation accuracy). <i>(Indicative time: about 30 minutes to code, runs in less than 15 seconds)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self, distance, k=1):\n",
    "        self.distance = distance\n",
    "        self.k = k\n",
    "    \n",
    "    '''\n",
    "    trains the distance and memorizes the training set\n",
    "    X: n x d (n samples of dimension d)\n",
    "    y: n (n labels)\n",
    "    '''\n",
    "    def fit(self, X, y):\n",
    "        # your code\n",
    "    \n",
    "    '''\n",
    "    predict the set of samples\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        #your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> <b>Q3.</b>  Implement a trainable $\\ell_2$ distance function. It performs a linear projection $P$ such that the distance between similar samples is minimized, trained using gradient descent:\n",
    "\n",
    "$$ \\min_P \\sum_{x, x_p, y = y_p} \\|Px - Px_p\\|^2 $$\n",
    "\n",
    "To reduce the cost of the update, perform the gradient descent on mini-batches of 50 samples taken randomly within the training set. Use 64 output dimensions. Plot the loss curve. <i>(Indicative time: about 30 minutes to code, runs for about 30 seconds for 2000 iterations)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2MinDistance(Distance):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.P = np.random.randn(input_dim, output_dim)/jnp.sqrt(input_dim+output_dim)\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def dist(X):\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(P, X, y):\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def update(P, X, y, eta=0.01):\n",
    "        l, dp = jax.value_and_grad(L2MinDistance.loss, argnums=0)(P, X, y)\n",
    "        return l, P - eta*dp\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # your code\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        # your code\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive loss\n",
    "\n",
    "Next, we want to mitigate the tendancy of only minimizing the distance to collapse all samples to the same location.\n",
    "\n",
    "<div class=\"alert alert-success\"> <b>Q4.</b> Code a trainable distance function that minimizes the distance between related samples up to a margin and maximizes the distance between unrelated samples up to a margin:\n",
    "\n",
    "$$\\min_p \\sum_{x, x_p, y = y_p} \\max(0, \\|Px - Px_p\\|^2 - \\alpha) - \\lambda \\sum_{x, x_n, y \\neq y_n} \\max(0, \\beta - \\|Px - Px_n\\|^2) $$\n",
    "\n",
    "Plot the loss value as the gradient descent progresses. To reduce the cost, perform the update on mini-batches of 50 samples taken randomly within the training set. Use again 64 output dimensions. <i>(Indicative time: 20 minutes to code, runs in about 45 seconds for 2000 iterations)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2ContrastiveDistance(Distance):\n",
    "    def __init__(self, input_dim, output_dim, alpha=0.1, beta= 0.95, lambd=0.75):\n",
    "        super().__init__()\n",
    "        # your code\n",
    "        \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def dist(X1, X2):\n",
    "        #your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(P, X, y, alpha, beta, lambd):\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def update(P, X, y, alpha, beta, lambd, eta=0.05):\n",
    "        l, dp = jax.value_and_grad(L2ContrastiveDistance.loss, argnums=0)(P, X, y, alpha, beta, lambd)\n",
    "        return l, P - eta*dp\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # your code\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        # your code       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet loss and non-linear projection\n",
    "\n",
    "There is nothing in the goals of metric learning that prevents us from using a non-linear projection $\\phi(\\cdot)$. On the contrary, using a non linear could allow us to have a mapping that contains implicit boundaries where on the one side samples would be pushed towards one direction, while on the other side they would be pushed towards another direction. To test this idea, we will use a simple non-linear mapping implemeted by a 2 layer MLP, which corresponds to the following formula:\n",
    "\n",
    "$$ \\phi(x) = P_2 [ P_1 x + b_1]_+ $$\n",
    "with $[\\cdot]_+$ the ReLU function.\n",
    "Instead of a single trainable argument, our projection method has 3 ($P_1, b_1, P_2$) and thus our loss function should also have 3, and the `argnums` argment of the gradient computation should also reflect this change (`argnums=(0,1,2)` for example).\n",
    "\n",
    "In addition, since $k$NN is in nature using a ranking approach by sorting the samples, we will know consider a loss function that enforces the order of the samples rather than their absolute distance values.\n",
    "\n",
    "$$ \\min_\\phi \\sum_{\\phi(x), \\phi(x_p), \\phi(x_n), y = y_p, y \\neq y_n} \\max(0, \\alpha + \\|\\phi(x) - \\phi(x_p)\\|^2 - \\|\\phi(x) - \\phi(x_n)\\|^2) $$\n",
    "\n",
    "<div class=\"alert alert-success\"> <b>Q5.</b> Code the non-linear triplet loss based trainable distance function. Similarly to other trainable distances, perform the update on mini-batches of size 50. Use a hidden size of 256 and again 64 output dimensions. <i>(Indicative time: 50 minutes to code, runs in about 2 minutes for 2000 iterations) and significantly improve the validation accuracy)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2TripletDistance(Distance):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, alpha=0.7):\n",
    "        super().__init__()\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def dist(X1, X2):\n",
    "        #your code\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def project(P1, b1, P2, X):\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def loss(P1, b1, P2, X, y, alpha):\n",
    "        # your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def update(P1, b1, P2, X, y, alpha, eta=0.05):\n",
    "        #your code\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # your code\n",
    "    \n",
    "    def predict(self, X1, X2):\n",
    "        # your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>Q6.</b> Perform a proper cross-validation on the $\\alpha$ parameter.</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyze your results in this box.</b>  Answer\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
