{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Numerical Algebra and Calculus tools in python\n",
    "\n",
    "In this TP, we will learn how to use (specific but not captive) linear algebra and calculus libraries in python. The objectives are:\n",
    "- To be able to translate a set of equations into python code\n",
    "- To be able to \"unroll\" for loops by using high order tensor operations\n",
    "- To be able to optimize a differentiable function by performing gradient descent using automatic differenciation tools\n",
    "- To be able to spot where the bottleneck is when translating a full algorithm into code\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Questions are in green boxes.</b>\n",
    "The maximum time you should spend on each question is given as indication only. If you take more time than that, then you should come see me.\n",
    "</div>\n",
    "<div class=\"alert alert-info\" role=\"alert\"><b>Analyzes are in blue boxes.</b> You should comment on your results in theses boxes (Is it good? Is it expected? Why do we get such result? Why is it different from the previous one? etc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Euclidean norm of a vector\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^d$ be a vector, we want to compute its squared $\\ell_2$ norm $\\|\\mathbf{x}\\|^2 = \\sum_i \\mathbf{x}[i]^2$.\n",
    "\n",
    "Let us first create $\\mathbf{x}$, with 1024 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q1.</b> Compute $\\|\\mathbf{x}\\|^2$ by converting the sum into a for loop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_sqnorm_for(x):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark the time it takes\n",
    "%timeit l2_sqnorm_for(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us note that there exists a _sum_ function in numpy (or in jax) and that the _*_ operator on arrays defaults to the element wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q2.</b> Write a new $\\ell_2$ norm function that performs element wise multiplication followed by using the sum function to avoid any for loop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_sqnorm_sum(x):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark it\n",
    "%timeit l2_sqnorm_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also note that the norm of $x$ is also equal to the dot product between $x$ and itself: $\\|x\\|^2 = \\langle x, x\\rangle$ which is accessible via the jnp.dot function.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q3.</b> write a new function using jnp.dot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_sqnorm_dot(x):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit l2_sqnorm_dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distance matrix\n",
    "\n",
    "Let us now consider sets of $n$ vectors of dimension $d$ arranged in a matrix $n \\times d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(64, 1024) # n = 64, d = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the matrix of size $n \\times n$ that contains the squared euclidean distance between every pair of samples: $D_{i,j} = \\|x_i - x_j\\|^2$.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q4.</b> Write a baseline function that computes the distance matrix between 2 sets of vector using for loops.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqdist_loop(x1, x2):\n",
    "    n1 = len(x1)\n",
    "    n2 = len(x2)\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sqdist_loop(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most linear algebra packages in python that emulate numpy support broadcasting. Brodcasting consist in manipulating arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. \n",
    "\n",
    "In our case, $x_1$ and $x_2$ have both size $n \\times d$. If we could extend the dimension of $x_1$ such that it has dimension $n\\times n \\times d$ and is repeated across the second dimension, and similarly extend $x_2$ such that it has size $n \\times n \\times d$ and is repeated across dimension 1, then $x_1 - x_2$ would be a 3 dimensional array where the position $[i,j,:]$ contains $x_1[i,:]  - x_2[j,:]$\n",
    "\n",
    "Fortunately, broadcasting avoids us the pain of manually replicating $x_1$ (resp $x_2$) across a new dimension. All we have to do is add a dimension of size 1 and the broadcast will do the replication. We can add a dimension by slicing None, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,None,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q5.</b> Write a function that has no for loops and instead uses broadcasting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqdist_bc(x1, x2):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sqdist_bc(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with brodcasting is that it may lead to gigantic memory consumption. In our case, we need to create the $n \\times n \\times d$ array in memory which does not scale with $n$ and takes time to allocate.\n",
    "\n",
    "Instead, we can recall that $\\|x_i - x_j\\|^2 = \\|x_i\\|^2 + \\|x_j\\|^2 - 2\\langle x_i, x_j \\rangle$. Combining that with broacasting, we can create a matrix $n\\times 1$ that contains all the square norms of $x_1$, a matrix $1\\times n$ containing the norms of $x_2$ and a matrix $n\\times n$ containing the dot product between all possible pairs and just add them all.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q6.</b>  Write a function using only dot products an no loop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqdist_dot(x1, x2):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sqdist_dot(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Einsum\n",
    "\n",
    "There is a tricky but practical function called ``jax.numpy.einsum`` that allows he user to perform summation over abritrary indices provided they have a matching number of dimension. For example, the regular matrix product between matrices A and B is defined as \n",
    "$$\n",
    "C_{ij} = \\sum_k A_{ik}B_{kj}\n",
    "$$\n",
    "and the corresponding einsum notation is then\n",
    "```\n",
    "C = einsum(\"ik, kj -> ij\", A, B)\n",
    "```\n",
    "which means that $A$ is indexed (in order) by $i$ and $k$, while $B$ is indexed by $k$ and $j$, and since the index $k$ is common between the 2 but absent from the output, the element wise product followed by the sum is performed on it.\n",
    "\n",
    "This can be extended to arbitrary number of indices (ex: `einsum(\"ijkl, mknj -> ilmn\",A, B)` will multiply then sum over the common indices $j$ and $k$ and then rearrange dimensions to match the output semantic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q7.</b>  Write a function using only calls to einsum.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqdist_einsum(x1, x2):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sqdist_einsum(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selection and partial updates\n",
    "\n",
    "In order to benefit also to the maximum of parallel computation, we also have to get rid of all conditional operations (if).\n",
    "\n",
    "Let us consider an example where we want to set to 0 all elements of an array that are above a specific threshold: $x[i] \\leftarrow x[i] \\text{ if } x[i] \\leq \\theta, 0$ else.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q8.</b> Write a baseline function that uses loop and if.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_loop(x, theta):\n",
    "    n, d = x.shape\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit thresh_loop(x, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of looping, we can use parallel operations to get the same result. Notice that the operation can be performed as the product of 2 arguments: $x[i] \\leftarrow x[i]\\mathbb{1}_{x[i]\\leq \\theta}$, with $\\mathbb{1}$ the indicator function.\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q9.</b>  Write a function without loop that only uses binary operation and products instead of if.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_bin(x, theta):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit thresh_bin(x, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization using gradient descent\n",
    "\n",
    "Machine learning relies a lot upon numerical optimization and gradient descent is one of the workhorse in that context. Python offers several toolkit with an autograd that allows us to compute the gradient of a function automatically, such as jax.\n",
    "\n",
    "As an example, we will optimize the following problem: $\\max_w w^\\top A w/|w\\|^2$, with $A$ p.s.d. The solution should correspond to an eigenvector of $A$ (rayleigh quotient).\n",
    "\n",
    "Let us define our objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, A):\n",
    "    return jnp.sum(w[None,:]@(A@w)/(w[None,:]@w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a value for $A$ and an initial for $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = x.T@x\n",
    "w0 = np.random.randn(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <b>Q10.</b> Write a function that returns the gradient of the objective function with respect to its first argument.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_grad(w, A):\n",
    "    # your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_value = []\n",
    "w=w0\n",
    "for ite in range(100):\n",
    "    w = w + 0.1*manual_grad(w, A)\n",
    "    loss_value.append(loss(w,A))\n",
    "plt.plot(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can use the jax autograd function to make sure we do not do any error in our derivation of the objective function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_grad = jax.grad(loss, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_value = []\n",
    "w = w0\n",
    "for ite in range(100):\n",
    "    w = w + 0.1*jax_grad(w, A)\n",
    "    loss_value.append(loss(w,A))\n",
    "plt.plot(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use jax to return the value and the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grad = jax.value_and_grad(loss, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_value = []\n",
    "w = w0\n",
    "for ite in range(100):\n",
    "    v, g = value_grad(w, A)\n",
    "    w = w + 0.1*g\n",
    "    loss_value.append(v)\n",
    "plt.plot(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why is it slower than our manual implementation? Well, everytime we call the function, it need to computes the gradient. Instead, we can ask jax to compile only once using annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(w, A):\n",
    "    return jnp.sum(w[None,:]@(A@w)/jnp.dot(w,w))\n",
    "\n",
    "value_grad = jax.value_and_grad(loss, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_value = []\n",
    "w = w0\n",
    "for ite in range(100):\n",
    "    v, g = value_grad(w, A)\n",
    "    w = w + 0.1*g\n",
    "    loss_value.append(v)\n",
    "plt.plot(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
